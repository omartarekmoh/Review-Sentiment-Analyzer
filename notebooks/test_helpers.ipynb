{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "        if token_to_idx == None:\n",
    "            token_to_idx = {}\n",
    "        self.token_to_idx = token_to_idx\n",
    "        self.idx_to_token = {idx : token for token, idx in self.token_to_idx.items()}\n",
    "        self.add_unk = add_unk\n",
    "        self.unk_token = unk_token\n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(self.unk_token)\n",
    "            \n",
    "    def add_token(self, token):\n",
    "        if token in self.token_to_idx:\n",
    "            return self.token_to_idx[token]\n",
    "        index = len(self.token_to_idx)\n",
    "        self.token_to_idx[token] =  index\n",
    "        self.idx_to_token[index] =  token\n",
    "        return index\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        if self.add_unk:\n",
    "            return self.token_to_idx.get(token, self.unk_index)\n",
    "        \n",
    "        return self.token_to_idx[token]\n",
    "    \n",
    "    def lookup_index(self, idx):\n",
    "        if idx in self.idx_to_token:\n",
    "            return self.idx_to_token[idx]\n",
    "        \n",
    "        raise KeyError(f\"The index ({idx}) isn't in the Vocabulary\")\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Vocabulary size: {len(self.token_to_idx)}\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        return {\n",
    "            \"token_to_idx\" : self.token_to_idx,\n",
    "            \"add_unk\" : self.add_unk,\n",
    "            \"unk_token\" : self.unk_token,\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewVectorizer:\n",
    "    def __init__(self, review_vocab, rating_vocab):\n",
    "        self.review_vocab = review_vocab\n",
    "        self.rating_vocab = rating_vocab\n",
    "    \n",
    "    def vectorize(self, review):\n",
    "        one_hot = np.zeros(len(self.review_vocab), dtype=np.float32)\n",
    "        \n",
    "        for word in review.split():\n",
    "            if word not in string.punctuation:\n",
    "                idx = self.review_vocab.lookup_token(word)\n",
    "                one_hot[idx] = 1\n",
    "                \n",
    "        return one_hot\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, dataframe, cut_off=25):\n",
    "        review_vocab = Vocabulary(add_unk=True)\n",
    "        rating_vocab = Vocabulary(add_unk=False)\n",
    "        \n",
    "        for rating in sorted(dataframe[\"ratings\"].unique()):\n",
    "            rating_vocab.add_token(rating)\n",
    "        \n",
    "        counter = Counter()\n",
    "        for _, row in dataframe.iterrows():\n",
    "            for word in row.reviews.split():\n",
    "                if word not in string.punctuation:\n",
    "                    counter[word] += 1\n",
    "        \n",
    "        for word, count in counter.items():\n",
    "            if count >= cut_off:\n",
    "                review_vocab.add_token(word)\n",
    "                \n",
    "        return cls(review_vocab, rating_vocab)\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        return {'review_vocab': self.review_vocab.to_serializable(),\n",
    "                'rating_vocab': self.rating_vocab.to_serializable()}\n",
    "        \n",
    "    @classmethod\n",
    "    def from_serializable(cls, content):\n",
    "        review_vocab = Vocabulary.from_serializable(content[\"review_vocab\"])\n",
    "        rating_vocab = Vocabulary.from_serializable(content[\"rating_vocab\"])\n",
    "        \n",
    "        return cls(review_vocab, rating_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, review_df, vectorizer):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.review_df = review_df\n",
    "        \n",
    "        self.train_df = self.review_df[self.review_df[\"split\"] == \"train\"]\n",
    "        self.train_size = len(train_df)\n",
    "        self.val_df = self.review_df[self.review_df[\"split\"] == \"val\"]\n",
    "        self.train_size = len(val_df)\n",
    "        self.test_df = self.review_df[self.review_df[\"split\"] == \"test\"]\n",
    "        self.train_size = len(test_df)\n",
    "\n",
    "        self.lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "        \n",
    "        self.set_split(\"train\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, review_csv):\n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        return cls(review_df, ReviewVectorizer.from_dataframe(review_df))\n",
    "    \n",
    "    def set_split(self, split):\n",
    "        self.target_split = split\n",
    "        self.target_df, self.target_size = self.lookup_dict[split]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.target_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.target_df.iloc[idx]\n",
    "        review_vector = self.vectorize.vectorize(row.reviews)\n",
    "        rating_index = self.vectorize.rating_vocab.lookup_token(row.ratings)\n",
    "        \n",
    "        return {\"x_data\" : review_vector,\n",
    "                \"y_target\" : rating_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
